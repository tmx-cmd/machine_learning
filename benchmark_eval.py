"""
SketchBench-10: Automated Evaluation for Sketch Generation
Features:
1. Objective Metric: Stroke Count Consistency (SVG Parsing)
2. Subjective Metric: Semantic & Aesthetic Scoring (Qwen3-VL)
"""

import os
import json
import base64
import time
import re
import requests
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from xml.dom import minidom
import numpy as np

# å¯¼å…¥ç”Ÿæˆæ¨¡å—
from clipasso_api import text_to_image

# ================= é…ç½®éƒ¨åˆ† =================
API_KEY = "sk-m9y9MfappohpJOyCvh8ZhA"  # ä½ çš„API Key
API_URL = "https://models.sjtu.edu.cn/api/v1/chat/completions"
MODEL_NAME = "qwen3vl"

OUTPUT_DIR = r"E:\mllab\machine_learning\sketch_benchmark_final"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å¤æ‚åº¦å¯¹åº”çš„é¢„æœŸç¬”ç”»æ•° (å‚è€ƒ clipasso_api.py ä¸­çš„å®šä¹‰)
COMPLEXITY_MAP = {
    "low": 16,
    "medium": 32,
    "high": 48
}

# ================= æ•°æ®é›† (SketchBench-10) =================
BENCHMARK_PROMPTS = [
    # --- ç®€å• (Low, 16 strokes) ---
    {"id": 1, "category": "Icon", "prompt": "An icon of a coffee mug", "complexity": "low"},
    {"id": 2, "category": "Icon", "prompt": "An apple fruit", "complexity": "low"},
    {"id": 3, "category": "Object", "prompt": "A simple desk lamp", "complexity": "low"},
    
    # --- ä¸­ç­‰ (Medium, 32 strokes) ---
    {"id": 4, "category": "Animal", "prompt": "A cute cat sitting", "complexity": "medium"},
    {"id": 5, "category": "Animal", "prompt": "A flying bird", "complexity": "medium"},
    {"id": 6, "category": "Food", "prompt": "A slice of pizza with pepperoni", "complexity": "medium"},
    {"id": 7, "category": "Plant", "prompt": "A blooming rose flower", "complexity": "medium"},

    # --- å¤æ‚ (High, 48 strokes) ---
    {"id": 8, "category": "Vehicle", "prompt": "A vintage car side view", "complexity": "high"},
    {"id": 9, "category": "Vehicle", "prompt": "A detailed bicycle", "complexity": "high"},
    {"id": 10, "category": "Animal", "prompt": "A galloping horse with details", "complexity": "high"},
]

# ================= è¾…åŠ©å‡½æ•° =================

def count_svg_strokes(svg_path):
    """
    è§£æSVGæ–‡ä»¶ï¼Œç²¾ç¡®è®¡ç®—ç¬”ç”»æ•° (<path> æ ‡ç­¾æ•°é‡)
    """
    try:
        if not os.path.exists(svg_path):
            return 0
        
        # æ–¹æ³•1: XMLè§£æ
        try:
            doc = minidom.parse(svg_path)
            paths = doc.getElementsByTagName('path')
            return len(paths)
        except:
            # æ–¹æ³•2: æ–‡æœ¬æ­£åˆ™è§£æ (å¤‡ç”¨)
            with open(svg_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content.count('<path')
    except Exception as e:
        print(f"Error parsing SVG {svg_path}: {e}")
        return 0

def svg_to_png(svg_path, png_path):
    """
    å°è¯•å°†SVGè½¬æ¢ä¸ºPNGä¾›å¤§æ¨¡å‹è¯„æµ‹ã€‚
    å¦‚æœç¼ºå°‘åº“ï¼Œåˆ™ç”Ÿæˆä¸€ä¸ªç©ºç™½å›¾æˆ–è¿”å›Falseã€‚
    """
    try:
        import cairosvg
        cairosvg.svg2png(url=svg_path, write_to=png_path)
        return True
    except ImportError:
        try:
            from svglib.svglib import svg2rlg
            from reportlab.graphics import renderPM
            drawing = svg2rlg(svg_path)
            renderPM.drawToFile(drawing, png_path, fmt="PNG")
            return True
        except ImportError:
            # print("Warning: cairosvg or svglib not installed. Cannot convert SVG for VL model.")
            return False
    except Exception as e:
        print(f"Conversion error: {e}")
        return False

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def evaluate_with_qwen(image_path, prompt, target_complexity):
    """è°ƒç”¨Qwen3-VLè¿›è¡Œå¤šç»´åº¦æ‰“åˆ†"""
    base64_image = encode_image(image_path)
    
    system_prompt = (
        "You are an expert art critic evaluating a computer-generated sketch. "
        "Assess the image strictly based on the provided metrics."
    )
    
    user_text = f"""
    Task: Evaluate this sketch for the prompt: "{prompt}".
    Target Complexity Level: {target_complexity.upper()}.
    
    Please rate (1-5) on these metrics:
    1. Semantic Alignment: Does it look like the object? (5=Perfect, 1=Unrecognizable)
    2. Sketch Esthetics: Is it a clean, artistic sketch (not a photo)? (5=Beautiful strokes, 1=Messy)
    3. Perceived Complexity: Does the visual detail match the target '{target_complexity}'? 
       (If target is LOW, it should be simple/minimal. If HIGH, it should be detailed. 5=Matches perfectly, 1=Complete mismatch)
    
    Return JSON only:
    {{
        "semantic_score": <int>,
        "esthetics_score": <int>,
        "complexity_match_score": <int>,
        "comment": "<short reasoning>"
    }}
    """
    
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}},
                {"type": "text", "text": user_text}
            ]}
        ],
        "temperature": 0.1,
        "stream": False
    }
    
    try:
        response = requests.post(API_URL, headers={
            "Content-Type": "application/json", 
            "Authorization": f"Bearer {API_KEY}"
        }, json=payload)
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            clean_content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_content)
    except Exception as e:
        print(f"API Error: {e}")
    
    return {"semantic_score": 0, "esthetics_score": 0, "complexity_match_score": 0, "comment": "Error"}

# ================= ä¸»ç¨‹åº =================

def run_benchmark():
    print(f"ğŸš€ Starting SketchBench-10 (Stroke Consistency & VL Eval)")
    results = []
    
    for item in tqdm(BENCHMARK_PROMPTS):
        # 1. è®¾ç½®è·¯å¾„
        case_name = f"case_{item['id']}_{item['complexity']}"
        case_dir = os.path.join(OUTPUT_DIR, case_name)
        
        # 2. ç”Ÿæˆ (Generation)
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ SVG
        svg_path = os.path.join(case_dir, f"best_sketch.svg") # å‡è®¾æ”¹åæˆ–å¤åˆ¶é€»è¾‘
        # æ³¨æ„ï¼šclipasso_api.py è¿”å›çš„æ˜¯ best_sketch_pathï¼Œå¯èƒ½æ˜¯å¸¦æ—¶é—´æˆ³çš„
        
        gen_data = None
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œåˆ™ç”Ÿæˆ
        if not os.path.exists(case_dir) or not any(f.endswith('.svg') for f in os.listdir(case_dir)):
            print(f"\nğŸ¨ Generating: {item['prompt']} ({item['complexity']})")
            gen_data = text_to_image(
                prompt=item['prompt'],
                complexity=item['complexity'],
                style="realistic", # ä¿æŒé£æ ¼ä¸€è‡´ä»¥æ§åˆ¶å˜é‡
                output_dir=case_dir,
                multiprocess=False
            )
            if gen_data and gen_data['success']:
                svg_source = gen_data['best_sketch_path']
                # ä¸ºäº†æ–¹ä¾¿åç»­å¤„ç†ï¼Œæ‰¾åˆ°ç”Ÿæˆçš„svg
                svg_path = svg_source
            else:
                print("Generation failed.")
                continue
        else:
            # æ‰¾åˆ°ç°æœ‰çš„SVG
            svg_files = [f for f in os.listdir(case_dir) if f.endswith('.svg')]
            if svg_files:
                svg_path = os.path.join(case_dir, svg_files[0])
                # æ¨¡æ‹Ÿä¸€ä¸ªæˆåŠŸè¿”å›
                gen_data = {"base_image_path": os.path.join(case_dir, "base.png")} 
            else:
                continue

        # 3. è¯„æµ‹æŒ‡æ ‡ A: ç¬”ç”»ä¸€è‡´æ€§ (Objective Stroke Consistency)
        actual_strokes = count_svg_strokes(svg_path)
        target_strokes = COMPLEXITY_MAP[item['complexity']]
        
        # è®¡ç®—è¯¯å·®ç‡ (Error Rate)
        stroke_diff = abs(actual_strokes - target_strokes)
        # ä¸€è‡´æ€§åˆ†æ•°ï¼š100% - å½’ä¸€åŒ–è¯¯å·®ã€‚å¦‚æœè¯¯å·®è¶…è¿‡ç›®æ ‡å€¼ï¼Œåˆ†æ•°ä¸º0
        stroke_consistency_score = max(0, 1 - (stroke_diff / target_strokes)) * 5.0 # æ˜ å°„åˆ° 5åˆ†åˆ¶
        
        print(f"   ğŸ“ Strokes: Actual={actual_strokes} | Target={target_strokes} | Score={stroke_consistency_score:.2f}/5")

        # 4. è¯„æµ‹æŒ‡æ ‡ B: è§†è§‰è´¨é‡ (Subjective VL Eval)
        # éœ€è¦å›¾ç‰‡æ–‡ä»¶ã€‚ä¼˜å…ˆç”¨ç”Ÿæˆçš„ SVG è½¬ PNGï¼Œå¦‚æœæ²¡æœ‰åº“ï¼Œåˆ™ç”¨ Base Image (ä»…ä½œå‚è€ƒ)
        
        eval_image_path = os.path.join(case_dir, "eval_preview.png")
        is_sketch_image = False
        
        if svg_to_png(svg_path, eval_image_path):
            is_sketch_image = True
        elif gen_data and "base_image_path" in gen_data and os.path.exists(gen_data["base_image_path"]):
            # é™çº§ï¼šå¦‚æœæ— æ³•è½¬SVGï¼Œä½¿ç”¨åº•å›¾è¯„æµ‹è¯­ä¹‰ï¼Œä½†åœ¨ prompt é‡Œå‘Šè¯‰æ¨¡å‹è¿™æ˜¯åº•å›¾
            eval_image_path = gen_data["base_image_path"]
            # æ³¨æ„ï¼šç”¨åº•å›¾è¯„æµ‹â€œç´ æç¾æ„Ÿâ€æ˜¯ä¸å‡†çš„ï¼Œæ‰€ä»¥è¿™é‡Œåªæ˜¯æƒå®œä¹‹è®¡
        else:
            eval_image_path = None

        vl_scores = {"semantic_score": 0, "esthetics_score": 0, "complexity_match_score": 0}
        
        if eval_image_path:
            # åªæœ‰å½“æ˜¯ç´ æå›¾æ—¶ï¼Œè¯„æµ‹æ‰æœ‰æ„ä¹‰ï¼›å¦‚æœæ˜¯åº•å›¾ï¼Œæˆ‘ä»¬åªå‚è€ƒè¯­ä¹‰
            vl_scores = evaluate_with_qwen(eval_image_path, item['prompt'], item['complexity'])
            if not is_sketch_image:
                vl_scores['esthetics_score'] = 0 # æƒ©ç½šï¼šæ— æ³•ç”Ÿæˆç´ æé¢„è§ˆ
                vl_scores['comment'] += " (Evaluated on base image due to missing SVG renderer)"

        # 5. æ±‡æ€»ç»“æœ
        row = item.copy()
        row.update({
            "actual_strokes": actual_strokes,
            "target_strokes": target_strokes,
            "stroke_consistency_score": stroke_consistency_score, # ç¡¬æŒ‡æ ‡
            "semantic_score": vl_scores['semantic_score'],        # è½¯æŒ‡æ ‡
            "esthetics_score": vl_scores['esthetics_score'],      # è½¯æŒ‡æ ‡
            "perceived_complexity_score": vl_scores['complexity_match_score'], # è½¯æŒ‡æ ‡
            "judge_comment": vl_scores.get('comment', '')
        })
        results.append(row)
        
        # é¿å…APIé™æµ
        time.sleep(1)

    # ================= æŠ¥å‘Šç”Ÿæˆ =================
    if not results:
        print("No results.")
        return

    df = pd.DataFrame(results)
    csv_path = os.path.join(OUTPUT_DIR, "final_benchmark_results.csv")
    df.to_csv(csv_path, index=False)
    
    # æ‰“å°æ§åˆ¶å°æŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š SketchBench-10 Final Report")
    print("="*80)
    
    # æ˜¾ç¤ºè¯¦ç»†åˆ†æ•°
    print(df[["prompt", "complexity", "actual_strokes", "stroke_consistency_score", "perceived_complexity_score"]])
    
    print("-" * 80)
    print(f"ğŸ† Overall Metrics (Average / 5.0):")
    print(f"   1. Stroke Consistency (Objective):  {df['stroke_consistency_score'].mean():.2f}")
    print(f"   2. Perceived Complexity (Subjective): {df['perceived_complexity_score'].mean():.2f}")
    print(f"   3. Semantic Alignment (AI Judge):   {df['semantic_score'].mean():.2f}")
    print(f"   4. Sketch Esthetics (AI Judge):     {df['esthetics_score'].mean():.2f}")
    print("="*80)
    print(f"ğŸ“„ Results saved to: {csv_path}")

    # ç®€å•çš„å¯è§†åŒ–ï¼šå¤æ‚åº¦ä¸€è‡´æ€§åˆ†æ
    try:
        plt.figure(figsize=(10, 6))
        # å½’ä¸€åŒ–å¹¶æ¯”è¾ƒ
        x = range(len(df))
        plt.bar(x, df['stroke_consistency_score'], width=0.4, label='Stroke Count Precision (Code)', align='center')
        plt.bar([i+0.4 for i in x], df['perceived_complexity_score'], width=0.4, label='Perceived Complexity (AI)', align='center')
        plt.xticks([i+0.2 for i in x], [p[:10]+"..." for p in df['prompt']], rotation=45)
        plt.legend()
        plt.title("Complexity Consistency: Objective (Strokes) vs Subjective (AI Vision)")
        plt.ylabel("Score (0-5)")
        plt.tight_layout()
        plt.savefig(os.path.join(OUTPUT_DIR, "complexity_analysis.png"))
        print("ğŸ“ˆ Chart generated.")
    except Exception as e:
        print(f"Plot error: {e}")

if __name__ == "__main__":
    run_benchmark()