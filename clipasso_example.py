"""
CLIPasso Multimodal Example (No External Dependencies)
Relies on pure prompt engineering for white background sketch generation.
"""

from clipasso_api import text_to_image
import os
import time

# å¯¼å…¥Stable Diffusionç›¸å…³åº“
try:
    from diffusers import StableDiffusionPipeline
    import torch
    STABLE_DIFFUSION_AVAILABLE = True
except ImportError:
    STABLE_DIFFUSION_AVAILABLE = False
    print("Warning: Stable Diffusion not available. Ablation study will be limited.")

def run_experiment(prompt, complexity, style, output_subdir):
    """Helper function to run a single experiment"""
    # è¯·ç¡®ä¿è¿™æ˜¯ä½ æƒ³è¦çš„è¾“å‡ºè·¯å¾„
    base_output_dir = r"E:\mllab\machine_learning\final_project_output_prompts_only"
    output_dir = os.path.join(base_output_dir, output_subdir)
    
    print(f"\n--- Starting Experiment: Style=[{style}], Complexity=[{complexity}] ---")
    print(f"Prompt: {prompt}")
    
    start_time = time.time()
    result = text_to_image(
        prompt=prompt,
        complexity=complexity,
        style=style,
        output_dir=output_dir,
        multiprocess=False
    )
    end_time = time.time()
    
    if result["success"]:
        print(f"âœ“ Success! (Time: {end_time - start_time:.2f}s)")
        print(f"  [Base Image]:  {result['base_image_path']}")
        print(f"  [Final Sketch]: {result['best_sketch_path']}")
        print("-" * 60)
    else:
        print(f"âœ— Failed: {result.get('error')}")

if __name__ == "__main__":
    import sys

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°ï¼Œå†³å®šè¿è¡Œå“ªä¸ªå®éªŒ
    if len(sys.argv) > 1 and sys.argv[1] == "ablation":
        # è¿è¡Œæ¶ˆèå®éªŒ
        ablation_study()
    else:
        # è¿è¡ŒåŸå§‹å®éªŒ
        print("è¿è¡ŒåŸå§‹å®éªŒç³»åˆ—...")
        print("å¦‚éœ€è¿è¡Œæ¶ˆèå®éªŒï¼Œè¯·ä½¿ç”¨: python clipasso_example.py ablation")

        # å®éªŒ 1: çŒ« (é«˜å¤æ‚åº¦ï¼Œå†™å®é£æ ¼)
        # æˆ‘ä»¬é€šè¿‡ Prompt å¼ºåˆ¶è¦æ±‚ "isolated on solid white background"
        run_experiment(
            prompt="A full body shot of a cute fluffy white persian cat sitting, looking at camera",
            complexity="high",
            style="realistic",
            output_subdir="exp1_cat_pure_prompt"
        )

        # å®éªŒ 2: åŠ¨æ¼«é£æ ¼ (ä¸­å¤æ‚åº¦)
        run_experiment(
            prompt="A magical warrior girl with a sword",
            complexity="medium",
            style="anime",
            output_subdir="exp2_anime_pure_prompt"
        )

        # å®éªŒ 3: ç®€å•å›¾æ ‡ (ä½å¤æ‚åº¦)
        run_experiment(
            prompt="An icon of a coffee cup",
            complexity="low",
            style="default",
            output_subdir="exp3_icon_pure_prompt"
        )

def ablation_study():
    """
    æ¶ˆèå®éªŒï¼šæ¯”è¾ƒä¸‰ç§æ–¹æ³•ç”Ÿæˆå›¾ç‰‡çš„æ•ˆæœ

    å®éªŒç»„åˆ«ï¼š
    1. ç›´æ¥ä½¿ç”¨Stable Diffusionç”Ÿæˆå›¾ç‰‡
    2. ä½¿ç”¨CLIPassoé¡¹ç›®ç”Ÿæˆå›¾ç‰‡ï¼ˆä¸ä½¿ç”¨promptå·¥ç¨‹ï¼‰
    3. ä½¿ç”¨CLIPassoé¡¹ç›®ç”Ÿæˆå›¾ç‰‡ï¼ˆä½¿ç”¨promptå·¥ç¨‹ï¼‰
    """

    if not STABLE_DIFFUSION_AVAILABLE:
        print("âŒ Stable Diffusionä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ¶ˆèå®éªŒ")
        print("è¯·å®‰è£…: pip install diffusers torch accelerate transformers")
        return

    # é…ç½®å®éªŒå‚æ•°
    base_prompt = "ä¸€åªå¯çˆ±çš„å°çŒ«"
    output_base_dir = r"E:\mllab\machine_learning\ablation_study"

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_base_dir, exist_ok=True)

    print("=" * 80)
    print("ğŸ¯ æ¶ˆèå®éªŒå¼€å§‹")
    print(f"ğŸ“ åŸºç¡€æç¤ºè¯: {base_prompt}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_base_dir}")
    print("=" * 80)

    # åˆå§‹åŒ–Stable Diffusionæ¨¡å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

    pipe = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    )
    pipe = pipe.to(device)

    if device == "cpu":
        pipe.enable_attention_slicing()

    # ========== å®éªŒç»„1: ç›´æ¥Stable Diffusionç”Ÿæˆ ==========
    print("\nğŸ”¬ å®éªŒç»„1: ç›´æ¥Stable Diffusionç”Ÿæˆ")
    print("-" * 50)

    group1_dir = os.path.join(output_base_dir, "group1_stable_diffusion_only")
    os.makedirs(group1_dir, exist_ok=True)

    print(f"ç”Ÿæˆå›¾ç‰‡: {base_prompt}")

    start_time = time.time()
    sd_image = pipe(
        prompt=base_prompt,
        negative_prompt="blurry, low quality, deformed, ugly",
        num_inference_steps=50,
        guidance_scale=7.5,
        width=512,
        height=512
    ).images[0]

    sd_output_path = os.path.join(group1_dir, "stable_diffusion_direct.png")
    sd_image.save(sd_output_path)
    end_time = time.time()

    print(f"âœ… å®Œæˆ! (è€—æ—¶: {end_time - start_time:.2f}s)")
    print(f"ğŸ“„ ä¿å­˜è·¯å¾„: {sd_output_path}")

    # ========== å®éªŒç»„2: CLIPassoé¡¹ç›®ï¼ˆä¸ä½¿ç”¨promptå·¥ç¨‹ï¼‰==========
    print("\nğŸ”¬ å®éªŒç»„2: CLIPassoé¡¹ç›®ï¼ˆä¸ä½¿ç”¨promptå·¥ç¨‹ï¼‰")
    print("-" * 50)

    group2_dir = os.path.join(output_base_dir, "group2_clipasso_no_prompt_engineering")

    print(f"ç”Ÿæˆç´ æ: {base_prompt} (åŸºç¡€æç¤ºè¯ï¼Œæ— é¢å¤–å·¥ç¨‹)")

    start_time = time.time()
    result2 = text_to_image(
        prompt=base_prompt,  # åªä½¿ç”¨åŸºç¡€æç¤ºè¯
        negative_prompt="",  # ä¸ä½¿ç”¨è´Ÿå‘æç¤ºè¯
        output_dir=group2_dir,
        num_strokes=16,      # åŸºç¡€ç¬”ç”»æ•°
        num_iter=1000,       # åŸºç¡€è¿­ä»£æ¬¡æ•°
        use_gpu=True,
        multiprocess=False
    )
    end_time = time.time()

    if result2["success"]:
        print(f"âœ… å®Œæˆ! (è€—æ—¶: {end_time - start_time:.2f}s)")
        print(f"ğŸ“„ ç´ æè·¯å¾„: {result2['best_sketch_path']}")
        print(f"ğŸ“„ åŸºç¡€å›¾åƒè·¯å¾„: {result2.get('base_image_temp_path', 'N/A')}")
    else:
        print(f"âŒ å¤±è´¥: {result2.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # ========== å®éªŒç»„3: CLIPassoé¡¹ç›®ï¼ˆä½¿ç”¨promptå·¥ç¨‹ï¼‰==========
    print("\nğŸ”¬ å®éªŒç»„3: CLIPassoé¡¹ç›®ï¼ˆä½¿ç”¨promptå·¥ç¨‹ï¼‰")
    print("-" * 50)

    group3_dir = os.path.join(output_base_dir, "group3_clipasso_with_prompt_engineering")

    # ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„promptå·¥ç¨‹
    engineered_prompt = (
        f"{base_prompt}ï¼Œå†™å®é£æ ¼ï¼Œé«˜æ¸…ç»†èŠ‚ï¼Œ"
        "ç™½è‰²èƒŒæ™¯ï¼Œå¹²å‡€ç®€æ´ï¼Œä¸“ä¸šæ’ç”»ï¼Œé«˜è´¨é‡ï¼Œ"
        "sharp focus, highly detailed, professional illustration"
    )

    engineered_negative = (
        "blurry, low quality, deformed, ugly, extra limbs, "
        "poor anatomy, watermark, text, signature, cartoon, anime"
    )

    print(f"ç”Ÿæˆç´ æ: {engineered_prompt}")
    print(f"è´Ÿå‘æç¤º: {engineered_negative}")

    start_time = time.time()
    result3 = text_to_image(
        prompt=engineered_prompt,
        negative_prompt=engineered_negative,
        output_dir=group3_dir,
        num_strokes=32,      # æ›´å¤šç¬”ç”»
        num_iter=2001,       # æ›´å¤šè¿­ä»£
        fix_scale=1,         # å›ºå®šæ¯”ä¾‹
        mask_object=1,       # é®ç½©èƒŒæ™¯
        use_gpu=True,
        multiprocess=False
    )
    end_time = time.time()

    if result3["success"]:
        print(f"âœ… å®Œæˆ! (è€—æ—¶: {end_time - start_time:.2f}s)")
        print(f"ğŸ“„ ç´ æè·¯å¾„: {result3['best_sketch_path']}")
        print(f"ğŸ“„ åŸºç¡€å›¾åƒè·¯å¾„: {result3.get('base_image_temp_path', 'N/A')}")
    else:
        print(f"âŒ å¤±è´¥: {result3.get('error', 'æœªçŸ¥é”™è¯¯')}")

    # ========== å®éªŒæ€»ç»“ ==========
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¶ˆèå®éªŒæ€»ç»“")
    print("=" * 80)

    print("å®éªŒç»„1 (Stable Diffusionç›´æ¥ç”Ÿæˆ):")
    print(f"  è·¯å¾„: {sd_output_path}")
    print("  ç‰¹ç‚¹: ç›´æ¥ç”Ÿæˆå½©è‰²å›¾åƒï¼Œæ— ç´ æè½¬æ¢")

    print("\nå®éªŒç»„2 (CLIPassoæ— promptå·¥ç¨‹):")
    if result2["success"]:
        print(f"  ç´ æè·¯å¾„: {result2['best_sketch_path']}")
        print("  ç‰¹ç‚¹: åŸºç¡€æç¤ºè¯ï¼Œæ ‡å‡†å‚æ•°è®¾ç½®")
    else:
        print("  çŠ¶æ€: ç”Ÿæˆå¤±è´¥")

    print("\nå®éªŒç»„3 (CLIPassoæœ‰promptå·¥ç¨‹):")
    if result3["success"]:
        print(f"  ç´ æè·¯å¾„: {result3['best_sketch_path']}")
        print("  ç‰¹ç‚¹: ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯ï¼Œæ›´é«˜è´¨é‡å‚æ•°")
    else:
        print("  çŠ¶æ€: ç”Ÿæˆå¤±è´¥")

    print(f"\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_base_dir}")
    print("ğŸ¯ å®éªŒå®Œæˆï¼è¯·æ¯”è¾ƒä¸‰ä¸ªå®éªŒç»„çš„ç»“æœã€‚")


# è¿è¡Œæ¶ˆèå®éªŒçš„ç¤ºä¾‹ä»£ç ï¼š
#
# # æ–¹æ³•1: å‘½ä»¤è¡Œè¿è¡Œ
# python clipasso_example.py ablation
#
# # æ–¹æ³•2: åœ¨Pythonä»£ç ä¸­è°ƒç”¨
# from clipasso_example import ablation_study
# ablation_study()